// Package scripts generates migration and maintenance scripts.
package scripts

import (
	"bytes"
	"fmt"
	"strings"
	"text/template"
	"time"

	"github.com/homeport/homeport/internal/domain/generator"
	"github.com/homeport/homeport/internal/domain/mapper"
	"github.com/homeport/homeport/internal/domain/resource"
)

// MigrationGenerator generates migration scripts for various AWS services.
type MigrationGenerator struct {
	projectName string
	region      string
}

// NewMigrationGenerator creates a new migration script generator.
func NewMigrationGenerator(projectName, region string) *MigrationGenerator {
	return &MigrationGenerator{
		projectName: projectName,
		region:      region,
	}
}

// Generate creates migration scripts based on the mapping results.
func (g *MigrationGenerator) Generate(results []*mapper.MappingResult) (*generator.Output, error) {
	output := generator.NewOutput()

	// Track which resource types we need to migrate
	resourceTypes := make(map[resource.Type]bool)
	for _, result := range results {
		// result.Scripts is now map[string][]byte
		for scriptName := range result.Scripts {
			// Infer resource type from script name
			if strings.Contains(scriptName, "s3") || strings.Contains(scriptName, "S3") {
				resourceTypes[resource.TypeS3Bucket] = true
			} else if strings.Contains(scriptName, "rds") || strings.Contains(scriptName, "RDS") {
				resourceTypes[resource.TypeRDSInstance] = true
			} else if strings.Contains(scriptName, "dynamodb") || strings.Contains(scriptName, "DynamoDB") {
				resourceTypes[resource.TypeDynamoDBTable] = true
			}
		}
	}

	// Generate S3 migration script
	if resourceTypes[resource.TypeS3Bucket] {
		script, err := g.generateS3Migration()
		if err != nil {
			return nil, fmt.Errorf("failed to generate S3 migration script: %w", err)
		}
		output.AddFile("scripts/migrate-s3.sh", []byte(script))
	}

	// Generate RDS migration script
	if resourceTypes[resource.TypeRDSInstance] || resourceTypes[resource.TypeRDSCluster] {
		script, err := g.generateRDSMigration()
		if err != nil {
			return nil, fmt.Errorf("failed to generate RDS migration script: %w", err)
		}
		output.AddFile("scripts/migrate-rds.sh", []byte(script))
	}

	// Generate DynamoDB migration script
	if resourceTypes[resource.TypeDynamoDBTable] {
		script, err := g.generateDynamoDBMigration()
		if err != nil {
			return nil, fmt.Errorf("failed to generate DynamoDB migration script: %w", err)
		}
		output.AddFile("scripts/migrate-dynamodb.sh", []byte(script))
	}

	// Generate master migration script
	masterScript, err := g.generateMasterMigration(resourceTypes)
	if err != nil {
		return nil, fmt.Errorf("failed to generate master migration script: %w", err)
	}
	output.AddFile("scripts/migrate.sh", []byte(masterScript))

	output.AddMetadata("generated_at", time.Now().Format(time.RFC3339))
	output.AddMetadata("project", g.projectName)

	return output, nil
}

// generateS3Migration generates a script to migrate S3 buckets to MinIO.
func (g *MigrationGenerator) generateS3Migration() (string, error) {
	tmpl := `#!/bin/bash
# S3 to MinIO Migration Script
# Generated by Homeport - {{.Timestamp}}
# Project: {{.ProjectName}}

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Configuration
AWS_REGION="${AWS_REGION:-{{.Region}}}"
MINIO_ENDPOINT="${MINIO_ENDPOINT:-http://localhost:9000}"
MINIO_ACCESS_KEY="${MINIO_ACCESS_KEY}"
MINIO_SECRET_KEY="${MINIO_SECRET_KEY}"

# Check prerequisites
echo -e "${YELLOW}Checking prerequisites...${NC}"
command -v aws >/dev/null 2>&1 || { echo -e "${RED}AWS CLI is required but not installed.${NC}" >&2; exit 1; }
command -v mc >/dev/null 2>&1 || { echo -e "${RED}MinIO Client (mc) is required but not installed.${NC}" >&2; exit 1; }

# Configure MinIO client
echo -e "${YELLOW}Configuring MinIO client...${NC}"
mc alias set minio "$MINIO_ENDPOINT" "$MINIO_ACCESS_KEY" "$MINIO_SECRET_KEY"
mc alias set aws https://s3.amazonaws.com "$AWS_ACCESS_KEY_ID" "$AWS_SECRET_ACCESS_KEY"

# Function to migrate a bucket
migrate_bucket() {
    local bucket_name=$1

    echo -e "${YELLOW}Migrating bucket: $bucket_name${NC}"

    # Check if bucket exists in S3
    if ! aws s3 ls "s3://$bucket_name" --region "$AWS_REGION" >/dev/null 2>&1; then
        echo -e "${RED}Bucket $bucket_name not found in S3${NC}"
        return 1
    fi

    # Create bucket in MinIO if it doesn't exist
    if ! mc ls "minio/$bucket_name" >/dev/null 2>&1; then
        echo -e "${YELLOW}Creating bucket in MinIO: $bucket_name${NC}"
        mc mb "minio/$bucket_name"
    fi

    # Get bucket policy
    echo -e "${YELLOW}Checking bucket policy...${NC}"
    if aws s3api get-bucket-policy --bucket "$bucket_name" --region "$AWS_REGION" --output text > "/tmp/${bucket_name}-policy.json" 2>/dev/null; then
        echo -e "${GREEN}Bucket policy saved to /tmp/${bucket_name}-policy.json${NC}"
        echo -e "${YELLOW}Note: You may need to manually apply this policy to MinIO${NC}"
    fi

    # Mirror bucket contents
    echo -e "${YELLOW}Syncing bucket contents...${NC}"
    mc mirror --preserve "aws/$bucket_name" "minio/$bucket_name" --watch &
    local mirror_pid=$!

    # Show progress
    while kill -0 $mirror_pid 2>/dev/null; do
        echo -n "."
        sleep 2
    done

    echo -e "\n${GREEN}Bucket $bucket_name migrated successfully${NC}"

    # Verify object count
    local s3_count=$(aws s3 ls "s3://$bucket_name" --recursive --region "$AWS_REGION" | wc -l)
    local minio_count=$(mc ls "minio/$bucket_name" --recursive | wc -l)

    echo -e "${YELLOW}S3 objects: $s3_count, MinIO objects: $minio_count${NC}"

    if [ "$s3_count" -eq "$minio_count" ]; then
        echo -e "${GREEN}Object counts match!${NC}"
    else
        echo -e "${RED}Warning: Object counts don't match${NC}"
    fi
}

# Main migration logic
echo -e "${GREEN}Starting S3 to MinIO migration${NC}"
echo -e "${YELLOW}This script will migrate your S3 buckets to MinIO${NC}"
echo ""

# Read buckets from environment or stdin
if [ -n "$S3_BUCKETS" ]; then
    IFS=',' read -ra BUCKETS <<< "$S3_BUCKETS"
    for bucket in "${BUCKETS[@]}"; do
        migrate_bucket "$bucket"
    done
else
    # List all buckets and ask for confirmation
    echo -e "${YELLOW}Available S3 buckets:${NC}"
    aws s3 ls --region "$AWS_REGION"
    echo ""
    read -p "Enter bucket names to migrate (comma-separated): " bucket_input
    IFS=',' read -ra BUCKETS <<< "$bucket_input"

    for bucket in "${BUCKETS[@]}"; do
        bucket=$(echo "$bucket" | xargs) # Trim whitespace
        migrate_bucket "$bucket"
    done
fi

echo -e "${GREEN}Migration complete!${NC}"
echo -e "${YELLOW}Please verify the migrated data before decommissioning S3 buckets.${NC}"
`

	data := struct {
		Timestamp   string
		ProjectName string
		Region      string
	}{
		Timestamp:   time.Now().Format(time.RFC3339),
		ProjectName: g.projectName,
		Region:      g.region,
	}

	t, err := template.New("s3-migration").Parse(tmpl)
	if err != nil {
		return "", err
	}

	var buf bytes.Buffer
	if err := t.Execute(&buf, data); err != nil {
		return "", err
	}

	return buf.String(), nil
}

// generateRDSMigration generates a script to migrate RDS databases.
func (g *MigrationGenerator) generateRDSMigration() (string, error) {
	tmpl := `#!/bin/bash
# RDS to PostgreSQL/MySQL Migration Script
# Generated by Homeport - {{.Timestamp}}
# Project: {{.ProjectName}}

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Configuration
RDS_ENDPOINT="${RDS_ENDPOINT}"
RDS_DATABASE="${RDS_DATABASE}"
RDS_USERNAME="${RDS_USERNAME}"
RDS_PASSWORD="${RDS_PASSWORD}"
DB_ENGINE="${DB_ENGINE:-postgres}" # postgres or mysql

TARGET_HOST="${TARGET_HOST:-localhost}"
TARGET_PORT="${TARGET_PORT:-5432}"
TARGET_DATABASE="${TARGET_DATABASE:-$RDS_DATABASE}"
TARGET_USERNAME="${TARGET_USERNAME:-$RDS_USERNAME}"
TARGET_PASSWORD="${TARGET_PASSWORD}"

BACKUP_DIR="${BACKUP_DIR:-./backups}"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Create backup directory
mkdir -p "$BACKUP_DIR"

echo -e "${GREEN}Starting RDS migration${NC}"
echo -e "${YELLOW}Engine: $DB_ENGINE${NC}"

# Function to migrate PostgreSQL
migrate_postgres() {
    echo -e "${YELLOW}Checking prerequisites...${NC}"
    command -v pg_dump >/dev/null 2>&1 || { echo -e "${RED}pg_dump is required but not installed.${NC}" >&2; exit 1; }
    command -v psql >/dev/null 2>&1 || { echo -e "${RED}psql is required but not installed.${NC}" >&2; exit 1; }

    echo -e "${YELLOW}Dumping RDS database...${NC}"
    PGPASSWORD="$RDS_PASSWORD" pg_dump \
        -h "$RDS_ENDPOINT" \
        -U "$RDS_USERNAME" \
        -d "$RDS_DATABASE" \
        -F c \
        -f "$BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}.dump" \
        --verbose

    echo -e "${GREEN}Dump completed: $BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}.dump${NC}"

    echo -e "${YELLOW}Restoring to target database...${NC}"
    PGPASSWORD="$TARGET_PASSWORD" pg_restore \
        -h "$TARGET_HOST" \
        -p "$TARGET_PORT" \
        -U "$TARGET_USERNAME" \
        -d "$TARGET_DATABASE" \
        -v \
        "$BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}.dump"

    echo -e "${GREEN}Restore completed${NC}"

    # Verify row counts
    echo -e "${YELLOW}Verifying migration...${NC}"
    echo "SELECT schemaname, tablename, n_live_tup FROM pg_stat_user_tables ORDER BY schemaname, tablename;" | \
        PGPASSWORD="$TARGET_PASSWORD" psql -h "$TARGET_HOST" -p "$TARGET_PORT" -U "$TARGET_USERNAME" -d "$TARGET_DATABASE"
}

# Function to migrate MySQL
migrate_mysql() {
    echo -e "${YELLOW}Checking prerequisites...${NC}"
    command -v mysqldump >/dev/null 2>&1 || { echo -e "${RED}mysqldump is required but not installed.${NC}" >&2; exit 1; }
    command -v mysql >/dev/null 2>&1 || { echo -e "${RED}mysql is required but not installed.${NC}" >&2; exit 1; }

    echo -e "${YELLOW}Dumping RDS database...${NC}"
    mysqldump \
        -h "$RDS_ENDPOINT" \
        -u "$RDS_USERNAME" \
        -p"$RDS_PASSWORD" \
        "$RDS_DATABASE" \
        --single-transaction \
        --routines \
        --triggers \
        --events \
        > "$BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}.sql"

    echo -e "${GREEN}Dump completed: $BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}.sql${NC}"

    echo -e "${YELLOW}Restoring to target database...${NC}"
    mysql \
        -h "$TARGET_HOST" \
        -P "$TARGET_PORT" \
        -u "$TARGET_USERNAME" \
        -p"$TARGET_PASSWORD" \
        "$TARGET_DATABASE" \
        < "$BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}.sql"

    echo -e "${GREEN}Restore completed${NC}"

    # Verify row counts
    echo -e "${YELLOW}Verifying migration...${NC}"
    mysql \
        -h "$TARGET_HOST" \
        -P "$TARGET_PORT" \
        -u "$TARGET_USERNAME" \
        -p"$TARGET_PASSWORD" \
        -e "SELECT table_schema, table_name, table_rows FROM information_schema.tables WHERE table_schema='$TARGET_DATABASE';" \
        "$TARGET_DATABASE"
}

# Execute migration based on engine type
case "$DB_ENGINE" in
    postgres|postgresql)
        migrate_postgres
        ;;
    mysql|mariadb)
        migrate_mysql
        ;;
    *)
        echo -e "${RED}Unsupported database engine: $DB_ENGINE${NC}"
        exit 1
        ;;
esac

echo -e "${GREEN}Migration complete!${NC}"
echo -e "${YELLOW}Backup saved to: $BACKUP_DIR/${RDS_DATABASE}_${TIMESTAMP}${NC}"
echo -e "${YELLOW}Please verify the migrated data before decommissioning RDS.${NC}"
`

	data := struct {
		Timestamp   string
		ProjectName string
	}{
		Timestamp:   time.Now().Format(time.RFC3339),
		ProjectName: g.projectName,
	}

	t, err := template.New("rds-migration").Parse(tmpl)
	if err != nil {
		return "", err
	}

	var buf bytes.Buffer
	if err := t.Execute(&buf, data); err != nil {
		return "", err
	}

	return buf.String(), nil
}

// generateDynamoDBMigration generates a script to migrate DynamoDB tables.
func (g *MigrationGenerator) generateDynamoDBMigration() (string, error) {
	tmpl := `#!/bin/bash
# DynamoDB Migration Script
# Generated by Homeport - {{.Timestamp}}
# Project: {{.ProjectName}}

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Configuration
AWS_REGION="${AWS_REGION:-us-east-1}"
DYNAMODB_ENDPOINT="${DYNAMODB_ENDPOINT:-http://localhost:8000}"
BACKUP_DIR="${BACKUP_DIR:-./backups/dynamodb}"
TIMESTAMP=$(date +%Y%m%d_%H%M%S)

# Create backup directory
mkdir -p "$BACKUP_DIR"

echo -e "${YELLOW}Checking prerequisites...${NC}"
command -v aws >/dev/null 2>&1 || { echo -e "${RED}AWS CLI is required but not installed.${NC}" >&2; exit 1; }

# Function to export table
export_table() {
    local table_name=$1
    local export_file="$BACKUP_DIR/${table_name}_${TIMESTAMP}.json"

    echo -e "${YELLOW}Exporting table: $table_name${NC}"

    # Scan and export all items
    aws dynamodb scan \
        --table-name "$table_name" \
        --region "$AWS_REGION" \
        --output json > "$export_file"

    local item_count=$(jq '.Items | length' "$export_file")
    echo -e "${GREEN}Exported $item_count items from $table_name${NC}"

    # Get table schema
    aws dynamodb describe-table \
        --table-name "$table_name" \
        --region "$AWS_REGION" \
        --output json > "$BACKUP_DIR/${table_name}_schema_${TIMESTAMP}.json"

    echo -e "${GREEN}Table schema saved${NC}"
}

# Function to import table
import_table() {
    local table_name=$1
    local import_file=$2
    local schema_file=$3

    echo -e "${YELLOW}Importing table: $table_name${NC}"

    # Create table from schema
    local create_params=$(jq '{
        TableName: .Table.TableName,
        AttributeDefinitions: .Table.AttributeDefinitions,
        KeySchema: .Table.KeySchema,
        BillingMode: "PAY_PER_REQUEST"
    }' "$schema_file")

    echo -e "${YELLOW}Creating table...${NC}"
    aws dynamodb create-table \
        --endpoint-url "$DYNAMODB_ENDPOINT" \
        --cli-input-json "$create_params" || echo -e "${YELLOW}Table may already exist${NC}"

    # Wait for table to be active
    echo -e "${YELLOW}Waiting for table to be active...${NC}"
    aws dynamodb wait table-exists \
        --endpoint-url "$DYNAMODB_ENDPOINT" \
        --table-name "$table_name"

    # Import items
    echo -e "${YELLOW}Importing items...${NC}"
    local items=$(jq -c '.Items[]' "$import_file")
    local count=0

    while IFS= read -r item; do
        aws dynamodb put-item \
            --endpoint-url "$DYNAMODB_ENDPOINT" \
            --table-name "$table_name" \
            --item "$item" >/dev/null

        count=$((count + 1))
        if [ $((count % 100)) -eq 0 ]; then
            echo -e "${YELLOW}Imported $count items...${NC}"
        fi
    done <<< "$items"

    echo -e "${GREEN}Imported $count items to $table_name${NC}"
}

# Main migration logic
echo -e "${GREEN}Starting DynamoDB migration${NC}"

# List tables
echo -e "${YELLOW}Listing DynamoDB tables...${NC}"
tables=$(aws dynamodb list-tables --region "$AWS_REGION" --output json | jq -r '.TableNames[]')

if [ -z "$tables" ]; then
    echo -e "${RED}No tables found${NC}"
    exit 1
fi

echo -e "${YELLOW}Found tables:${NC}"
echo "$tables"
echo ""

# Export all tables
for table in $tables; do
    export_table "$table"
done

echo -e "${GREEN}Export complete!${NC}"
echo -e "${YELLOW}To import to local DynamoDB, run:${NC}"
echo ""
for table in $tables; do
    echo "  import_table '$table' '$BACKUP_DIR/${table}_${TIMESTAMP}.json' '$BACKUP_DIR/${table}_schema_${TIMESTAMP}.json'"
done
echo ""
echo -e "${YELLOW}Backups saved to: $BACKUP_DIR${NC}"
`

	data := struct {
		Timestamp   string
		ProjectName string
	}{
		Timestamp:   time.Now().Format(time.RFC3339),
		ProjectName: g.projectName,
	}

	t, err := template.New("dynamodb-migration").Parse(tmpl)
	if err != nil {
		return "", err
	}

	var buf bytes.Buffer
	if err := t.Execute(&buf, data); err != nil {
		return "", err
	}

	return buf.String(), nil
}

// generateMasterMigration generates a master script that orchestrates all migrations.
func (g *MigrationGenerator) generateMasterMigration(resourceTypes map[resource.Type]bool) (string, error) {
	var buf bytes.Buffer

	buf.WriteString("#!/bin/bash\n")
	buf.WriteString("# Master Migration Script\n")
	buf.WriteString(fmt.Sprintf("# Generated by Homeport - %s\n", time.Now().Format(time.RFC3339)))
	buf.WriteString(fmt.Sprintf("# Project: %s\n\n", g.projectName))

	buf.WriteString("set -e\n\n")

	buf.WriteString("# Colors for output\n")
	buf.WriteString("RED='\\033[0;31m'\n")
	buf.WriteString("GREEN='\\033[0;32m'\n")
	buf.WriteString("YELLOW='\\033[1;33m'\n")
	buf.WriteString("NC='\\033[0m' # No Color\n\n")

	buf.WriteString("echo -e \"${GREEN}Homeport Migration Suite${NC}\"\n")
	buf.WriteString("echo -e \"${YELLOW}This will migrate your AWS resources to self-hosted alternatives${NC}\"\n")
	buf.WriteString("echo \"\"\n\n")

	buf.WriteString("# Change to script directory\n")
	buf.WriteString("cd \"$(dirname \"$0\")\"\n\n")

	if resourceTypes[resource.TypeS3Bucket] {
		buf.WriteString("# Migrate S3 buckets\n")
		buf.WriteString("echo -e \"${YELLOW}Step 1: Migrating S3 buckets to MinIO${NC}\"\n")
		buf.WriteString("if [ -f ./migrate-s3.sh ]; then\n")
		buf.WriteString("    chmod +x ./migrate-s3.sh\n")
		buf.WriteString("    ./migrate-s3.sh\n")
		buf.WriteString("else\n")
		buf.WriteString("    echo -e \"${RED}migrate-s3.sh not found${NC}\"\n")
		buf.WriteString("fi\n")
		buf.WriteString("echo \"\"\n\n")
	}

	if resourceTypes[resource.TypeRDSInstance] {
		buf.WriteString("# Migrate RDS databases\n")
		buf.WriteString("echo -e \"${YELLOW}Step 2: Migrating RDS databases${NC}\"\n")
		buf.WriteString("if [ -f ./migrate-rds.sh ]; then\n")
		buf.WriteString("    chmod +x ./migrate-rds.sh\n")
		buf.WriteString("    ./migrate-rds.sh\n")
		buf.WriteString("else\n")
		buf.WriteString("    echo -e \"${RED}migrate-rds.sh not found${NC}\"\n")
		buf.WriteString("fi\n")
		buf.WriteString("echo \"\"\n\n")
	}

	if resourceTypes[resource.TypeDynamoDBTable] {
		buf.WriteString("# Migrate DynamoDB tables\n")
		buf.WriteString("echo -e \"${YELLOW}Step 3: Migrating DynamoDB tables${NC}\"\n")
		buf.WriteString("if [ -f ./migrate-dynamodb.sh ]; then\n")
		buf.WriteString("    chmod +x ./migrate-dynamodb.sh\n")
		buf.WriteString("    ./migrate-dynamodb.sh\n")
		buf.WriteString("else\n")
		buf.WriteString("    echo -e \"${RED}migrate-dynamodb.sh not found${NC}\"\n")
		buf.WriteString("fi\n")
		buf.WriteString("echo \"\"\n\n")
	}

	buf.WriteString("echo -e \"${GREEN}Migration complete!${NC}\"\n")
	buf.WriteString("echo -e \"${YELLOW}Please review the migration logs and verify all data before decommissioning AWS resources.${NC}\"\n")

	return buf.String(), nil
}
