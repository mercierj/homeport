// Package hetzner provides storage resource generation for Hetzner Cloud.
package hetzner

import (
	"bytes"
	"fmt"
	"strings"

	"github.com/homeport/homeport/internal/domain/generator"
	"github.com/homeport/homeport/internal/domain/mapper"
	"github.com/homeport/homeport/internal/domain/resource"
)

// HetznerVolumePrice is the price per GB per month for Hetzner volumes (in EUR).
const HetznerVolumePrice = 0.044

// MinVolumeSize is the minimum volume size in GB.
const MinVolumeSize = 10

// MaxVolumeSize is the maximum volume size in GB.
const MaxVolumeSize = 10240

// GenerateStorageTF generates Terraform configuration for storage resources.
func GenerateStorageTF(categorized *CategorizedResults, config *generator.TargetConfig, location string) string {
	var buf bytes.Buffer

	buf.WriteString("# Hetzner Cloud Storage Resources\n")
	buf.WriteString("# Generated by Homeport\n\n")

	// Track volume names to avoid duplicates
	volumeNames := make(map[string]bool)

	// Generate volumes from block storage resources
	for i, result := range categorized.Storage {
		if result == nil {
			continue
		}

		name := SanitizeName(result.SourceResourceName)
		if name == "" {
			name = fmt.Sprintf("volume-%d", i)
		}

		// Ensure unique names
		baseName := name
		counter := 1
		for volumeNames[name] {
			name = fmt.Sprintf("%s-%d", baseName, counter)
			counter++
		}
		volumeNames[name] = true

		size := estimateVolumeSize(result)
		buf.WriteString(generateVolumeResource(name, size, result))
	}

	// Generate data volumes for databases if needed
	for i, result := range categorized.Database {
		if result == nil {
			continue
		}

		name := fmt.Sprintf("db-%s", SanitizeName(result.SourceResourceName))
		if name == "db-" {
			name = fmt.Sprintf("db-data-%d", i)
		}

		// Ensure unique names
		baseName := name
		counter := 1
		for volumeNames[name] {
			name = fmt.Sprintf("%s-%d", baseName, counter)
			counter++
		}
		volumeNames[name] = true

		size := estimateDatabaseVolumeSize(result)
		buf.WriteString(generateDatabaseVolumeResource(name, size))
	}

	// Generate MinIO configuration for S3-compatible object storage
	hasObjectStorage := false
	for _, result := range categorized.Storage {
		if result != nil && result.SourceCategory == resource.CategoryObjectStorage {
			hasObjectStorage = true
			break
		}
	}

	if hasObjectStorage {
		buf.WriteString(generateMinIOConfig(config))
	}

	// Add storage variables
	buf.WriteString("\n# Storage variables\n")
	buf.WriteString(`variable "volume_filesystem" {
  description = "Filesystem type for volumes"
  type        = string
  default     = "ext4"

  validation {
    condition     = contains(["ext4", "xfs"], var.volume_filesystem)
    error_message = "Filesystem must be ext4 or xfs."
  }
}

variable "enable_volume_backups" {
  description = "Enable automated volume snapshots"
  type        = bool
  default     = true
}

variable "volume_backup_retention" {
  description = "Number of volume snapshots to retain"
  type        = number
  default     = 7
}
`)

	return buf.String()
}

// generateVolumeResource generates a Terraform hcloud_volume resource.
func generateVolumeResource(name string, size int, result *mapper.MappingResult) string {
	var buf bytes.Buffer

	comment := fmt.Sprintf("# Volume: %s", name)
	if result != nil && result.SourceResourceType != "" {
		comment = fmt.Sprintf("# Volume: %s (migrated from %s)", name, result.SourceResourceType)
	}

	buf.WriteString(comment + "\n")
	buf.WriteString(fmt.Sprintf(`resource "hcloud_volume" "%s" {
  name      = "${local.project_name}-%s"
  size      = %d
  location  = local.location
  format    = var.volume_filesystem
  automount = false

  labels = merge(local.common_labels, {
    purpose = "data"
  })

  lifecycle {
    prevent_destroy = false
  }
}

resource "hcloud_volume_attachment" "%s" {
  volume_id = hcloud_volume.%s.id
  server_id = hcloud_server.app[0].id
  automount = true
}

`, name, name, size, name, name))

	return buf.String()
}

// generateDatabaseVolumeResource generates a volume resource for database storage.
func generateDatabaseVolumeResource(name string, size int) string {
	var buf bytes.Buffer

	buf.WriteString(fmt.Sprintf("# Database volume: %s\n", name))
	buf.WriteString(fmt.Sprintf(`resource "hcloud_volume" "%s" {
  name      = "${local.project_name}-%s"
  size      = %d
  location  = local.location
  format    = var.volume_filesystem
  automount = false

  labels = merge(local.common_labels, {
    purpose = "database"
  })

  lifecycle {
    prevent_destroy = true  # Protect database volumes
  }
}

resource "hcloud_volume_attachment" "%s" {
  volume_id = hcloud_volume.%s.id
  server_id = var.enable_dedicated_db ? hcloud_server.database[0].id : hcloud_server.app[0].id
  automount = true
}

`, name, name, size, name, name))

	return buf.String()
}

// generateMinIOConfig generates MinIO configuration for S3-compatible object storage.
func generateMinIOConfig(config *generator.TargetConfig) string {
	var buf bytes.Buffer

	buf.WriteString("\n# MinIO for S3-compatible Object Storage\n")
	buf.WriteString("# MinIO is deployed via Docker on the app servers\n\n")

	// Add MinIO-specific volumes
	buf.WriteString(`resource "hcloud_volume" "minio_data" {
  count     = var.enable_minio ? 1 : 0
  name      = "${local.project_name}-minio-data"
  size      = var.minio_volume_size
  location  = local.location
  format    = "ext4"
  automount = false

  labels = merge(local.common_labels, {
    purpose = "object-storage"
  })
}

resource "hcloud_volume_attachment" "minio_data" {
  count     = var.enable_minio ? 1 : 0
  volume_id = hcloud_volume.minio_data[0].id
  server_id = hcloud_server.app[0].id
  automount = true
}

# MinIO configuration variables
variable "enable_minio" {
  description = "Enable MinIO for S3-compatible object storage"
  type        = bool
  default     = true
}

variable "minio_volume_size" {
  description = "Size of MinIO data volume in GB"
  type        = number
  default     = 100
}

variable "minio_access_key" {
  description = "MinIO access key"
  type        = string
  sensitive   = true
  default     = ""
}

variable "minio_secret_key" {
  description = "MinIO secret key"
  type        = string
  sensitive   = true
  default     = ""
}

`)

	// For HA deployments, configure distributed MinIO
	if config.HALevel.RequiresMultiServer() {
		buf.WriteString(`# For HA: MinIO runs in distributed mode across servers
# Each server gets a data volume for MinIO

resource "hcloud_volume" "minio_distributed" {
  count     = var.enable_minio && var.server_count > 1 ? var.server_count : 0
  name      = "${local.project_name}-minio-${count.index}"
  size      = var.minio_volume_size
  location  = local.location
  format    = "ext4"
  automount = false

  labels = merge(local.common_labels, {
    purpose = "object-storage"
    node    = count.index
  })
}

resource "hcloud_volume_attachment" "minio_distributed" {
  count     = var.enable_minio && var.server_count > 1 ? var.server_count : 0
  volume_id = hcloud_volume.minio_distributed[count.index].id
  server_id = hcloud_server.app[count.index].id
  automount = true
}
`)
	}

	return buf.String()
}

// estimateVolumeSize estimates the required volume size based on the source resource.
func estimateVolumeSize(result *mapper.MappingResult) int {
	if result == nil {
		return 50 // Default 50GB
	}

	resourceType := strings.ToLower(result.SourceResourceType)

	// EBS volume sizing
	if strings.Contains(resourceType, "ebs") {
		if strings.Contains(resourceType, "gp3") || strings.Contains(resourceType, "gp2") {
			return 50 // General purpose
		}
		if strings.Contains(resourceType, "io1") || strings.Contains(resourceType, "io2") {
			return 100 // High IOPS
		}
		if strings.Contains(resourceType, "st1") || strings.Contains(resourceType, "sc1") {
			return 200 // Throughput optimized
		}
	}

	// GCP disk sizing
	if strings.Contains(resourceType, "persistent_disk") {
		if strings.Contains(resourceType, "ssd") {
			return 100
		}
		return 50
	}

	// Azure disk sizing
	if strings.Contains(resourceType, "managed_disk") {
		return 100
	}

	return 50 // Default
}

// estimateDatabaseVolumeSize estimates volume size for database storage.
func estimateDatabaseVolumeSize(result *mapper.MappingResult) int {
	if result == nil {
		return 50 // Default 50GB for database
	}

	resourceType := strings.ToLower(result.SourceResourceType)

	// RDS sizing
	if strings.Contains(resourceType, "rds") {
		if strings.Contains(resourceType, "large") {
			return 200
		}
		if strings.Contains(resourceType, "medium") {
			return 100
		}
		return 50
	}

	// DynamoDB (estimate based on typical usage)
	if strings.Contains(resourceType, "dynamodb") {
		return 100
	}

	// ElastiCache
	if strings.Contains(resourceType, "elasticache") || strings.Contains(resourceType, "redis") {
		return 20 // Redis doesn't need much disk
	}

	// CloudSQL
	if strings.Contains(resourceType, "sql_database") || strings.Contains(resourceType, "cloudsql") {
		return 100
	}

	return 50 // Default
}

// CalculateStorageCost calculates monthly storage cost.
func CalculateStorageCost(totalGB int) float64 {
	return float64(totalGB) * HetznerVolumePrice
}

// GenerateVolumeBackupConfig generates configuration for volume snapshots.
func GenerateVolumeBackupConfig() string {
	return `# Volume Backup Configuration
# Note: Hetzner Cloud doesn't have automated snapshots via Terraform
# Use a cron job or external tool for automated backups

# Example backup script (create manually on server):
# #!/bin/bash
# VOLUME_ID=$(hcloud volume list -o noheader -o columns=id | head -1)
# hcloud volume create-snapshot $VOLUME_ID --name "backup-$(date +%Y%m%d)"
#
# # Cleanup old snapshots (keep last 7)
# hcloud volume snapshot list --volume $VOLUME_ID -o noheader -o columns=id | tail -n +8 | xargs -I {} hcloud volume snapshot delete {}

# For automated backups, consider:
# 1. restic with Hetzner Storage Box
# 2. Borg Backup to Hetzner Storage Box
# 3. Custom snapshot script with hcloud CLI
`
}

// MapCloudStorageToHetzner maps cloud storage types to Hetzner volumes.
func MapCloudStorageToHetzner(sourceType string) (string, int) {
	sourceType = strings.ToLower(sourceType)

	// AWS S3 -> MinIO
	if strings.Contains(sourceType, "s3") {
		return "minio", 100
	}

	// AWS EBS -> Hetzner Volume
	if strings.Contains(sourceType, "ebs") {
		return "volume", 50
	}

	// AWS EFS -> Hetzner Volume (shared via NFS)
	if strings.Contains(sourceType, "efs") {
		return "volume", 100
	}

	// GCS -> MinIO
	if strings.Contains(sourceType, "storage_bucket") || strings.Contains(sourceType, "gcs") {
		return "minio", 100
	}

	// Azure Blob -> MinIO
	if strings.Contains(sourceType, "blob") || strings.Contains(sourceType, "storage_account") {
		return "minio", 100
	}

	// Azure Disk -> Hetzner Volume
	if strings.Contains(sourceType, "managed_disk") {
		return "volume", 100
	}

	return "volume", 50
}
