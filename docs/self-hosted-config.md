# Self-Hosted Configuration Guides

This document provides detailed configuration guides for all self-hosted services generated by Homeport.

## Table of Contents

- [Traefik Reverse Proxy](#traefik-reverse-proxy)
- [PostgreSQL Database](#postgresql-database)
- [MySQL Database](#mysql-database)
- [MinIO Object Storage](#minio-object-storage)
- [Redis Cache](#redis-cache)
- [RabbitMQ Message Broker](#rabbitmq-message-broker)
- [Keycloak Authentication](#keycloak-authentication)
- [ScyllaDB (DynamoDB Alternative)](#scylladb-dynamodb-alternative)
- [OpenFaaS (Lambda Alternative)](#openfaas-lambda-alternative)
- [Monitoring Stack](#monitoring-stack)
- [Backup Configuration](#backup-configuration)
- [Security Hardening](#security-hardening)
- [High Availability Setup](#high-availability-setup)

---

## Traefik Reverse Proxy

Traefik serves as the entry point for all HTTP/HTTPS traffic, handling SSL termination, load balancing, and routing.

### Basic Configuration

Generated file: `traefik/traefik.yml`

```yaml
api:
  dashboard: true
  insecure: false  # Set to false in production

entryPoints:
  web:
    address: ":80"
    http:
      redirections:
        entryPoint:
          to: websecure
          scheme: https

  websecure:
    address: ":443"
    http:
      tls:
        certResolver: letsencrypt

providers:
  docker:
    endpoint: "unix:///var/run/docker.sock"
    exposedByDefault: false
    network: homeport

  file:
    directory: /etc/traefik/dynamic
    watch: true

certificatesResolvers:
  letsencrypt:
    acme:
      email: admin@example.com
      storage: /letsencrypt/acme.json
      httpChallenge:
        entryPoint: web
```

### SSL Configuration Options

#### Let's Encrypt (Recommended)

```yaml
certificatesResolvers:
  letsencrypt:
    acme:
      email: admin@example.com
      storage: /letsencrypt/acme.json
      # Production server (default)
      # caServer: https://acme-v02.api.letsencrypt.org/directory
      # Staging server (for testing)
      # caServer: https://acme-staging-v02.api.letsencrypt.org/directory
      httpChallenge:
        entryPoint: web
```

#### DNS Challenge (Wildcard Certificates)

```yaml
certificatesResolvers:
  letsencrypt:
    acme:
      email: admin@example.com
      storage: /letsencrypt/acme.json
      dnsChallenge:
        provider: cloudflare
        resolvers:
          - "1.1.1.1:53"
          - "8.8.8.8:53"
```

Environment variables for DNS providers:
```bash
# Cloudflare
CF_API_EMAIL=your@email.com
CF_API_KEY=your-api-key

# AWS Route53
AWS_ACCESS_KEY_ID=xxx
AWS_SECRET_ACCESS_KEY=xxx
AWS_REGION=us-east-1
```

#### Custom Certificates

```yaml
# traefik/dynamic/certs.yml
tls:
  certificates:
    - certFile: /certs/example.com.crt
      keyFile: /certs/example.com.key
```

### Dynamic Routing

Create service routes in `traefik/dynamic/`:

```yaml
# traefik/dynamic/services.yml
http:
  routers:
    webapp:
      rule: "Host(`app.example.com`)"
      service: webapp
      tls:
        certResolver: letsencrypt
      middlewares:
        - security-headers
        - rate-limit

    api:
      rule: "Host(`api.example.com`)"
      service: api
      tls:
        certResolver: letsencrypt

  services:
    webapp:
      loadBalancer:
        servers:
          - url: "http://webapp:8080"

    api:
      loadBalancer:
        servers:
          - url: "http://api:3000"

  middlewares:
    security-headers:
      headers:
        stsSeconds: 31536000
        stsIncludeSubdomains: true
        stsPreload: true
        forceSTSHeader: true
        contentTypeNosniff: true
        browserXssFilter: true
        referrerPolicy: "strict-origin-when-cross-origin"
        customFrameOptionsValue: "SAMEORIGIN"

    rate-limit:
      rateLimit:
        average: 100
        burst: 50
```

### Docker Labels for Services

```yaml
services:
  myapp:
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.myapp.rule=Host(`app.example.com`)"
      - "traefik.http.routers.myapp.entrypoints=websecure"
      - "traefik.http.routers.myapp.tls.certresolver=letsencrypt"
      - "traefik.http.services.myapp.loadbalancer.server.port=8080"
```

### Dashboard Access

Secure the Traefik dashboard:

```yaml
# traefik/dynamic/dashboard.yml
http:
  routers:
    dashboard:
      rule: "Host(`traefik.example.com`)"
      service: api@internal
      middlewares:
        - auth
      tls:
        certResolver: letsencrypt

  middlewares:
    auth:
      basicAuth:
        users:
          # Generate with: htpasswd -nb admin password
          - "admin:$apr1$xxx"
```

---

## PostgreSQL Database

### Basic Configuration

```yaml
# docker-compose.yml
services:
  postgres:
    image: postgres:16-alpine
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-webapp}
      POSTGRES_USER: ${POSTGRES_USER:-webapp}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD required}
    volumes:
      - ./data/postgres:/var/lib/postgresql/data
      - ./configs/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
      - ./configs/postgres/postgresql.conf:/etc/postgresql/postgresql.conf
    command: postgres -c config_file=/etc/postgresql/postgresql.conf
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-webapp}"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - homeport
```

### Performance Tuning

Create `configs/postgres/postgresql.conf`:

```ini
# Memory Settings (adjust based on available RAM)
shared_buffers = 256MB              # 25% of RAM for dedicated DB server
effective_cache_size = 768MB        # 75% of RAM
work_mem = 16MB
maintenance_work_mem = 64MB

# Write Ahead Log
wal_buffers = 16MB
checkpoint_completion_target = 0.9
wal_level = replica                 # For replication support

# Query Planner
random_page_cost = 1.1              # SSD-optimized
effective_io_concurrency = 200      # SSD-optimized

# Connections
max_connections = 100
max_parallel_workers_per_gather = 2
max_worker_processes = 4

# Logging
log_destination = 'stderr'
logging_collector = on
log_directory = 'log'
log_filename = 'postgresql-%Y-%m-%d.log'
log_statement = 'ddl'
log_min_duration_statement = 1000   # Log queries > 1s

# Locale
datestyle = 'iso, mdy'
timezone = 'UTC'
lc_messages = 'en_US.utf8'
lc_monetary = 'en_US.utf8'
lc_numeric = 'en_US.utf8'
lc_time = 'en_US.utf8'
```

### Connection Pooling with PgBouncer

For high-traffic applications, add PgBouncer:

```yaml
services:
  pgbouncer:
    image: edoburu/pgbouncer:1.21.0
    environment:
      DATABASE_URL: postgres://webapp:${POSTGRES_PASSWORD}@postgres:5432/webapp
      POOL_MODE: transaction
      MAX_CLIENT_CONN: 1000
      DEFAULT_POOL_SIZE: 20
    ports:
      - "6432:5432"
    depends_on:
      - postgres
    networks:
      - homeport
```

### Backup Configuration

Automated backups with pg_dump:

```bash
#!/bin/bash
# scripts/backup-postgres.sh

BACKUP_DIR=/backups/postgres
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_FILE="${BACKUP_DIR}/backup_${DATE}.sql.gz"

# Create backup
docker compose exec -T postgres pg_dumpall -U webapp | gzip > "$BACKUP_FILE"

# Keep only last 7 days
find "$BACKUP_DIR" -name "*.sql.gz" -mtime +7 -delete

echo "Backup created: $BACKUP_FILE"
```

Add to crontab:
```bash
0 2 * * * /path/to/scripts/backup-postgres.sh
```

---

## MySQL Database

### Basic Configuration

```yaml
services:
  mysql:
    image: mysql:8.0
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD:?required}
      MYSQL_DATABASE: ${MYSQL_DATABASE:-webapp}
      MYSQL_USER: ${MYSQL_USER:-webapp}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD:?required}
    volumes:
      - ./data/mysql:/var/lib/mysql
      - ./configs/mysql/my.cnf:/etc/mysql/conf.d/custom.cnf
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - homeport
```

### Performance Tuning

Create `configs/mysql/my.cnf`:

```ini
[mysqld]
# InnoDB Settings
innodb_buffer_pool_size = 256M
innodb_log_file_size = 64M
innodb_flush_log_at_trx_commit = 2
innodb_flush_method = O_DIRECT

# Query Cache (disabled in MySQL 8+, use ProxySQL)
# query_cache_type = 0

# Connections
max_connections = 151
thread_cache_size = 8

# Logging
slow_query_log = 1
slow_query_log_file = /var/log/mysql/slow.log
long_query_time = 2

# Character Set
character-set-server = utf8mb4
collation-server = utf8mb4_unicode_ci

# Timezone
default-time-zone = '+00:00'
```

---

## MinIO Object Storage

### Basic Configuration

```yaml
services:
  minio:
    image: quay.io/minio/minio:latest
    command: server /data --console-address ":9001"
    restart: unless-stopped
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:?required}
      MINIO_BROWSER_REDIRECT_URL: https://minio-console.example.com
    volumes:
      - ./data/minio:/data
    ports:
      - "9001:9001"  # Console (remove in production, use Traefik)
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 5s
      retries: 3
    networks:
      - homeport
    labels:
      - "traefik.enable=true"
      # API endpoint
      - "traefik.http.routers.minio-api.rule=Host(`s3.example.com`)"
      - "traefik.http.routers.minio-api.service=minio-api"
      - "traefik.http.services.minio-api.loadbalancer.server.port=9000"
      # Console
      - "traefik.http.routers.minio-console.rule=Host(`minio.example.com`)"
      - "traefik.http.routers.minio-console.service=minio-console"
      - "traefik.http.services.minio-console.loadbalancer.server.port=9001"
```

### Bucket Configuration

Create buckets and policies using MinIO Client:

```bash
# Configure alias
mc alias set myminio https://s3.example.com $MINIO_ROOT_USER $MINIO_ROOT_PASSWORD

# Create buckets
mc mb myminio/uploads
mc mb myminio/backups

# Set bucket policies
mc anonymous set download myminio/uploads/public/

# Create service account for application
mc admin user add myminio app-user app-secret-key
mc admin policy attach myminio readwrite --user app-user
```

### Lifecycle Rules

```bash
# Expire old objects
mc ilm add --expiry-days 30 myminio/temp-uploads

# Transition to different storage class
mc ilm add --transition-days 90 --storage-class GLACIER myminio/archives
```

### Bucket Notifications

Configure event notifications:

```bash
# Notify on object creation
mc event add myminio/uploads arn:minio:sqs::1:webhook \
  --event put \
  --suffix .jpg
```

---

## Redis Cache

### Basic Configuration

```yaml
services:
  redis:
    image: redis:7-alpine
    restart: unless-stopped
    command: redis-server /usr/local/etc/redis/redis.conf
    volumes:
      - ./data/redis:/data
      - ./configs/redis/redis.conf:/usr/local/etc/redis/redis.conf
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - homeport
```

### Configuration File

Create `configs/redis/redis.conf`:

```ini
# Network
bind 0.0.0.0
port 6379
protected-mode yes

# Authentication
requirepass ${REDIS_PASSWORD}

# Memory Management
maxmemory 256mb
maxmemory-policy allkeys-lru

# Persistence
appendonly yes
appendfsync everysec
dir /data

# Snapshotting
save 900 1
save 300 10
save 60 10000

# Logging
loglevel notice
logfile ""

# Performance
tcp-backlog 511
tcp-keepalive 300
```

### Redis Sentinel (High Availability)

For production, add Redis Sentinel:

```yaml
services:
  redis-master:
    image: redis:7-alpine
    command: redis-server --requirepass ${REDIS_PASSWORD}
    networks:
      - homeport

  redis-slave:
    image: redis:7-alpine
    command: redis-server --replicaof redis-master 6379 --masterauth ${REDIS_PASSWORD} --requirepass ${REDIS_PASSWORD}
    depends_on:
      - redis-master
    networks:
      - homeport

  redis-sentinel:
    image: redis:7-alpine
    command: redis-sentinel /etc/redis/sentinel.conf
    volumes:
      - ./configs/redis/sentinel.conf:/etc/redis/sentinel.conf
    networks:
      - homeport
```

Sentinel configuration (`configs/redis/sentinel.conf`):

```ini
sentinel monitor mymaster redis-master 6379 2
sentinel auth-pass mymaster ${REDIS_PASSWORD}
sentinel down-after-milliseconds mymaster 5000
sentinel failover-timeout mymaster 60000
sentinel parallel-syncs mymaster 1
```

---

## RabbitMQ Message Broker

### Basic Configuration

```yaml
services:
  rabbitmq:
    image: rabbitmq:3.12-management-alpine
    restart: unless-stopped
    environment:
      RABBITMQ_DEFAULT_USER: ${RABBITMQ_USER:-admin}
      RABBITMQ_DEFAULT_PASS: ${RABBITMQ_PASSWORD:?required}
      RABBITMQ_DEFAULT_VHOST: /
    volumes:
      - ./data/rabbitmq:/var/lib/rabbitmq
      - ./configs/rabbitmq/rabbitmq.conf:/etc/rabbitmq/rabbitmq.conf
      - ./configs/rabbitmq/definitions.json:/etc/rabbitmq/definitions.json
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "-q", "ping"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - homeport
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.rabbitmq.rule=Host(`rabbitmq.example.com`)"
      - "traefik.http.services.rabbitmq.loadbalancer.server.port=15672"
```

### Configuration File

Create `configs/rabbitmq/rabbitmq.conf`:

```ini
# Default user
default_user = admin
default_pass = changeme
default_vhost = /

# Memory
vm_memory_high_watermark.relative = 0.7
vm_memory_high_watermark_paging_ratio = 0.5

# Disk
disk_free_limit.absolute = 1GB

# Logging
log.console = true
log.console.level = info
log.file = /var/log/rabbitmq/rabbit.log
log.file.level = info

# Management
management.load_definitions = /etc/rabbitmq/definitions.json
management.tcp.port = 15672

# Clustering (if needed)
# cluster_formation.peer_discovery_backend = rabbit_peer_discovery_consul
```

### Queue Definitions

Create `configs/rabbitmq/definitions.json`:

```json
{
  "vhosts": [
    {"name": "/"}
  ],
  "users": [
    {
      "name": "admin",
      "password_hash": "generated-hash",
      "tags": "administrator"
    },
    {
      "name": "app",
      "password_hash": "generated-hash",
      "tags": ""
    }
  ],
  "permissions": [
    {
      "user": "app",
      "vhost": "/",
      "configure": ".*",
      "write": ".*",
      "read": ".*"
    }
  ],
  "exchanges": [
    {
      "name": "events",
      "vhost": "/",
      "type": "topic",
      "durable": true
    },
    {
      "name": "dlx",
      "vhost": "/",
      "type": "fanout",
      "durable": true
    }
  ],
  "queues": [
    {
      "name": "orders",
      "vhost": "/",
      "durable": true,
      "arguments": {
        "x-dead-letter-exchange": "dlx",
        "x-message-ttl": 86400000
      }
    },
    {
      "name": "notifications",
      "vhost": "/",
      "durable": true
    },
    {
      "name": "dead-letters",
      "vhost": "/",
      "durable": true
    }
  ],
  "bindings": [
    {
      "source": "events",
      "vhost": "/",
      "destination": "orders",
      "destination_type": "queue",
      "routing_key": "order.*"
    },
    {
      "source": "dlx",
      "vhost": "/",
      "destination": "dead-letters",
      "destination_type": "queue"
    }
  ]
}
```

---

## Keycloak Authentication

### Basic Configuration

```yaml
services:
  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    command: start --optimized
    restart: unless-stopped
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: ${KEYCLOAK_DB_PASSWORD:?required}
      KC_HOSTNAME: auth.example.com
      KC_PROXY: edge
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: ${KEYCLOAK_ADMIN_PASSWORD:?required}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - homeport
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.keycloak.rule=Host(`auth.example.com`)"
      - "traefik.http.services.keycloak.loadbalancer.server.port=8080"
```

### Realm Configuration

Export and import realms:

```bash
# Export realm
docker compose exec keycloak /opt/keycloak/bin/kc.sh export \
  --dir /tmp/export \
  --realm myapp

# Import realm
docker compose exec keycloak /opt/keycloak/bin/kc.sh import \
  --dir /tmp/import
```

### OIDC Client Configuration

Create a client for your application:

```json
{
  "clientId": "my-app",
  "name": "My Application",
  "protocol": "openid-connect",
  "publicClient": false,
  "secret": "your-client-secret",
  "redirectUris": [
    "https://app.example.com/callback",
    "https://app.example.com/silent-refresh.html"
  ],
  "webOrigins": [
    "https://app.example.com"
  ],
  "standardFlowEnabled": true,
  "directAccessGrantsEnabled": true,
  "serviceAccountsEnabled": true
}
```

### Application Integration

Example environment variables:

```bash
OIDC_ISSUER=https://auth.example.com/realms/myapp
OIDC_CLIENT_ID=my-app
OIDC_CLIENT_SECRET=your-client-secret
OIDC_REDIRECT_URI=https://app.example.com/callback
```

---

## ScyllaDB (DynamoDB Alternative)

### Basic Configuration

```yaml
services:
  scylladb:
    image: scylladb/scylla:5.4
    command: >
      --alternator-port=8000
      --alternator-write-isolation=always
      --smp 2
      --memory 1G
    restart: unless-stopped
    volumes:
      - ./data/scylla:/var/lib/scylla
    ports:
      - "9042:9042"   # CQL
      - "8000:8000"   # DynamoDB API
    healthcheck:
      test: ["CMD-SHELL", "nodetool status"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - homeport
```

### Application Configuration

```python
import boto3

# Configure for ScyllaDB Alternator
dynamodb = boto3.resource(
    'dynamodb',
    endpoint_url='http://scylladb:8000',
    region_name='us-east-1',  # Can be any value
    aws_access_key_id='none',
    aws_secret_access_key='none'
)
```

---

## OpenFaaS (Lambda Alternative)

### Basic Configuration

```yaml
services:
  gateway:
    image: ghcr.io/openfaas/gateway:latest
    environment:
      functions_provider_url: "http://faas-swarm:8080/"
      read_timeout: "5m"
      write_timeout: "5m"
      upstream_timeout: "5m30s"
      faas_prometheus_host: "prometheus"
      faas_prometheus_port: "9090"
    ports:
      - "8080:8080"
    networks:
      - homeport

  queue-worker:
    image: ghcr.io/openfaas/queue-worker:latest
    environment:
      faas_gateway_address: "gateway:8080"
      faas_function_suffix: ""
      ack_wait: "5m5s"
    networks:
      - homeport
```

### Deploying Functions

```bash
# Login
echo $PASSWORD | faas-cli login --password-stdin

# Deploy function
faas-cli deploy -f stack.yml
```

---

## Monitoring Stack

### Prometheus Configuration

```yaml
services:
  prometheus:
    image: prom/prometheus:v2.48.0
    restart: unless-stopped
    volumes:
      - ./configs/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./data/prometheus:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
    networks:
      - homeport
```

Create `configs/prometheus/prometheus.yml`:

```yaml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

scrape_configs:
  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

  - job_name: 'traefik'
    static_configs:
      - targets: ['traefik:8082']

  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']

  - job_name: 'redis'
    static_configs:
      - targets: ['redis-exporter:9121']

  - job_name: 'node'
    static_configs:
      - targets: ['node-exporter:9100']
```

### Grafana Configuration

```yaml
services:
  grafana:
    image: grafana/grafana:10.2.2
    restart: unless-stopped
    environment:
      GF_SECURITY_ADMIN_USER: admin
      GF_SECURITY_ADMIN_PASSWORD: ${GRAFANA_PASSWORD:?required}
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_SERVER_ROOT_URL: https://grafana.example.com
    volumes:
      - ./data/grafana:/var/lib/grafana
      - ./configs/grafana/provisioning:/etc/grafana/provisioning
    networks:
      - homeport
```

---

## Backup Configuration

### Automated Backup Script

Create `scripts/backup.sh`:

```bash
#!/bin/bash
set -euo pipefail

BACKUP_DIR=${BACKUP_DIR:-/backups}
DATE=$(date +%Y%m%d_%H%M%S)
RETENTION_DAYS=${RETENTION_DAYS:-7}

mkdir -p "$BACKUP_DIR"/{postgres,redis,minio}

echo "Starting backup at $(date)"

# PostgreSQL
echo "Backing up PostgreSQL..."
docker compose exec -T postgres pg_dumpall -U webapp | \
  gzip > "$BACKUP_DIR/postgres/backup_$DATE.sql.gz"

# Redis
echo "Backing up Redis..."
docker compose exec -T redis redis-cli BGSAVE
sleep 5
cp ./data/redis/dump.rdb "$BACKUP_DIR/redis/backup_$DATE.rdb"

# MinIO
echo "Backing up MinIO metadata..."
mc mirror myminio "$BACKUP_DIR/minio/backup_$DATE" --preserve

# Cleanup old backups
echo "Cleaning up old backups..."
find "$BACKUP_DIR" -type f -mtime +$RETENTION_DAYS -delete

echo "Backup completed at $(date)"
```

### Cron Schedule

```bash
# Daily backup at 2 AM
0 2 * * * /path/to/scripts/backup.sh >> /var/log/backup.log 2>&1

# Weekly full backup on Sunday
0 3 * * 0 /path/to/scripts/full-backup.sh >> /var/log/backup.log 2>&1
```

---

## Security Hardening

### Docker Security

```yaml
# docker-compose.yml security settings
services:
  myapp:
    security_opt:
      - no-new-privileges:true
    read_only: true
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
```

### Network Isolation

```yaml
networks:
  homeport:
    driver: bridge
    internal: false

  backend:
    driver: bridge
    internal: true  # No external access

services:
  postgres:
    networks:
      - backend  # Only internal access

  webapp:
    networks:
      - homeport
      - backend
```

### Secrets Management

Use Docker secrets or environment files:

```yaml
services:
  postgres:
    secrets:
      - postgres_password
    environment:
      POSTGRES_PASSWORD_FILE: /run/secrets/postgres_password

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
```

---

## High Availability Setup

### Database Replication

See [Migration Guides](migration-guides.md) for detailed replication setup.

### Load Balancing

Traefik automatically load balances across multiple instances:

```yaml
services:
  webapp:
    deploy:
      replicas: 3
    labels:
      - "traefik.enable=true"
      - "traefik.http.services.webapp.loadbalancer.server.port=8080"
      - "traefik.http.services.webapp.loadbalancer.healthcheck.path=/health"
      - "traefik.http.services.webapp.loadbalancer.healthcheck.interval=10s"
```

### Health Checks

Ensure all services have proper health checks:

```yaml
healthcheck:
  test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
  interval: 30s
  timeout: 10s
  retries: 3
  start_period: 40s
```
